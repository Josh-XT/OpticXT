# CUDA Debugging Improvements for OpticXT

## üîß Changes Made

### 1. **Enhanced CUDA Detection**
- Added `is_cuda_available()` method that checks multiple indicators:
  - `nvidia-smi` command availability and output
  - CUDA environment variables (CUDA_VISIBLE_DEVICES)
  - CUDA library presence in standard paths
- GPU information logging during detection

### 2. **Explicit Device Environment Setup**
- Set `CUDA_VISIBLE_DEVICES=0` to force GPU 0 usage
- Set `CUDA_LAUNCH_BLOCKING=1` for better debugging
- Added environment variable logging

### 3. **GPU Memory Monitoring**
- `log_gpu_memory_usage()` method shows VRAM usage before/after model loading
- Memory tracking throughout the inference pipeline
- Detailed GPU information logging (name, memory usage)

### 4. **GPU Utilization Monitoring**
- `check_gpu_utilization()` method monitors GPU usage during inference
- 5-second monitoring window to catch brief GPU activity
- Shows both GPU and memory utilization percentages

### 5. **Better Error Reporting**
- More detailed error messages for CUDA failures
- Distinction between CUDA detection vs CUDA model loading failures
- Explicit logging when falling back to CPU

## üöÄ Usage

### Option 1: Use the debug script
```bash
./debug_cuda.sh
```

### Option 2: Manual testing with environment variables
```bash
export CUDA_VISIBLE_DEVICES=0
export CUDA_LAUNCH_BLOCKING=1  
export RUST_LOG=debug
cargo run -- --test-simple
```

### Option 3: Check GPU status manually
```bash
# Check GPU before running
nvidia-smi

# Check GPU during model loading (in another terminal)
watch -n 1 nvidia-smi

# Check GPU utilization specifically
nvidia-smi --query-gpu=utilization.gpu,utilization.memory --format=csv -l 1
```

## üîç What to Look For

### **Signs that CUDA is working:**
- Log message: "‚úÖ Successfully loaded UQFF model on CUDA device"
- GPU memory usage increases after model loading
- GPU utilization shows >0% during inference
- VRAM usage visible in nvidia-smi

### **Signs that it's falling back to CPU:**
- Log message: "‚ùå CUDA UQFF loading failed"
- No change in GPU memory usage
- 100% CPU usage during inference
- GPU utilization stays at 0%

## üõ†Ô∏è Troubleshooting

If CUDA is detected but inference still uses CPU:
1. Check if mistralrs was compiled with CUDA support
2. Verify CUDA toolkit version compatibility
3. Check if the model format supports GPU acceleration
4. Monitor memory usage - might be running out of VRAM

The RTX 4090 has 24GB VRAM which should be more than enough for this model, so if GPU memory isn't increasing, the issue is likely in the model loading/device selection logic.
